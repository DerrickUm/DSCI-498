{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Multi-dimensional arrays and datasets\n",
    "import xarray as xr\n",
    "# Geospatial raster data handling\n",
    "import rioxarray as rxr\n",
    "# Geospatial data analysis\n",
    "import geopandas as gpd\n",
    "# Geospatial operations\n",
    "import rasterio\n",
    "from rasterio import windows  \n",
    "from rasterio import features  \n",
    "from rasterio import warp\n",
    "from rasterio.warp import transform_bounds \n",
    "from rasterio.windows import from_bounds \n",
    "# Image Processing\n",
    "from PIL import Image\n",
    "# Coordinate transformations\n",
    "from pyproj import Proj, Transformer, CRS\n",
    "# Feature Engineering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "# Planetary Computer Tools\n",
    "import pystac_client\n",
    "import planetary_computer as pc\n",
    "from pystac.extensions.eo import EOExtension as eo\n",
    "# Others\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban Heat Island (UHI) Benchmark Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\">Welcome to the EY Open Science AI & Data Challenge 2025! The objective of this challenge is to build a machine learning model to predict urban heat island (UHI) hotspots in a city. By the end of the challenge, you will have developed a regression model capable of predicting the intensity of the UHI effect.\n",
    "\n",
    "Participants will be given ground-level air temperature data in an index format, which was collected on 24th July 2021 on traverse points in the Bronx and Manhattan regions of New York city. This dataset constitutes traverse points (latitude and longitude) and their corresponding UHI (Urban Heat Island) index values. Participants will use this dataset to build a regression model to predict UHI index values for a given set of locations. It is important to understand that the UHI Index at any given location is indicative of the relative temperature elevation at that specific point compared to the city's average temperature.\n",
    "\n",
    "This challenge is designed for participants with varying skill levels in data science and programming, offering a great opportunity to apply your knowledge and enhance your capabilities in the field.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Challenge Aim: </b><p align=\"justify\"> <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\">In this notebook, we will demonstrate a basic model workflow that can serve as a starting point for the challenge. The basic model has been constructed to predict the Urban Heat Island (UHI) index using features from the Sentinel-2 satellite dataset as predictor variables. In this demonstration, we utilized three features from the Sentinel-2 dataset: band B01 (Coastal Aerosol), band B06 (Red Edge), and NDVI (Normalized Difference Vegetation Index) derived from bands B04 (Red) and B08 (Near Infrared). A random forest regression model was then trained using these features.\n",
    "    \n",
    "These features were extracted from a GeoTIFF image created by the Sentinel-2 sample notebook. For the sample model shown in this notebook, data from a single day (24th July 2021) was considered, assuming that the values of bands B01, B04, B06, and B08 for this specific date are representative of the UHI index behavior at any location. Participants should review the details of the Sentinel-2 sample notebook to gain an understanding of the data and options for modifying the output product. \n",
    "    \n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">Most of the functions presented in this notebook were adapted from the <a href=\"https://planetarycomputer.microsoft.com/dataset/sentinel-2-l2a#Example-Notebook\">Sentinel-2-Level-2A notebook</a> found in the Planetary Computer portal.</p>\n",
    "\n",
    "<p align=\"justify\">Please note that this notebook is just a starting point. We have made many assumptions in this notebook that you may think are not best for solving the challenge effectively. You are encouraged to modify these functions, rewrite them, or try an entirely new approach.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load In Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this demonstration notebook, you will need to have the following packages imported below installed. This may take some time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Multi-dimensional arrays and datasets\n",
    "import xarray as xr\n",
    "\n",
    "# Geospatial raster data handling\n",
    "import rioxarray as rxr\n",
    "\n",
    "# Geospatial data analysis\n",
    "import geopandas as gpd\n",
    "\n",
    "# Geospatial operations\n",
    "import rasterio\n",
    "from rasterio import windows  \n",
    "from rasterio import features  \n",
    "from rasterio import warp\n",
    "from rasterio.warp import transform_bounds \n",
    "from rasterio.windows import from_bounds \n",
    "\n",
    "# Image Processing\n",
    "from PIL import Image\n",
    "\n",
    "# Coordinate transformations\n",
    "from pyproj import Proj, Transformer, CRS\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Planetary Computer Tools\n",
    "import pystac_client\n",
    "import planetary_computer as pc\n",
    "from pystac.extensions.eo import EOExtension as eo\n",
    "\n",
    "# Others\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the model, we need to load in the Urban Heat Island (UHI) index training dataset. We have curated data for the New York region. The dataset consists of geo-locations (Longitude and Latitude), with additional fields including date & time of data collection and the UHI index for each location. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h10 style=\"color:rgb(255, 255, 0)\"><strong>Note: Participants are strictly prohibited from using Longitude and Latitude values as features in building their machine learning models. These values should only be utilized for understanding the attributes and characteristics of the locations. Submissions that employ longitude and latitude values as model features will be disqualified.</strong></h10>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h10 style=\"color:rgb(255, 255, 0)\"><strong>Incorporating latitude and longitude data in their raw forms or through any form of manipulation—including multiplication, embedding, or conversion to polar coordinates—as predictive features in your model is strictly prohibited, as it can compromise the adaptability of your model across diverse scenarios. This prohibition extends to calculating the distance from a reference point and using it as a feature, which is essentially a transformation of the original geographical coordinates into a new feature form. Submissions that include these types of features will be considered non-compliant and will be disqualified.</strong></h10>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data from csv file and display the first few rows to inspect the data\n",
    "ground_df = pd.read_csv(\"Training_data_uhi_index.csv\")\n",
    "ground_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\">Now that we have our UHI data, it is time to gather the predictor variables from the Sentinel-2 dataset. Participants should review the provided Sentinel-2 sample notebook as it was used to create a sample GeoTIFF for this models. For a more in-depth look regarding the Sentinel-2 dataset and how to query it, see the Microsoft Planetary Computer example <a href=\"https://planetarycomputer.microsoft.com/dataset/sentinel-2-l2a#Example-Notebook\">Sentinel-2 supplementary notebook</a>. </p>\n",
    "\n",
    "<p align=\"justify\">Sentinel-2 optical data provides high-resolution imagery that is sensitive to land surface characteristics, which are crucial for understanding urban heat dynamics. Band values such as B01 (Coastal aerosol), B06 (Red Edge), and NDVI (Normalized Difference Vegetation Index) derived from B04 (Red) and B08 (Near Infrared) help us in estimating the UHI index. Hence, we are choosing B01, B06, and NDVI as predictor variables for this experiment.</p>\n",
    "\n",
    "<ul> \n",
    "<li>B01 - Reflectance values from the Coastal aerosol band, which help in assessing aerosol presence and improving atmospheric correction.</li>\n",
    "\n",
    "<li>B06 - Reflectance values from the Red Edge band, which provide useful information for detecting vegetation, water bodies, and urban surfaces.</li>\n",
    "\n",
    "<li>NDVI - Derived from B04 (Red) and B08 (Near Infrared), NDVI is an important indicator for vegetation health and land cover.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(255, 255, 0)\"><strong>Tip 1</strong></h4>\n",
    "<p align=\"justify\">Participants might explore other combinations of bands from the Sentinel-2 and from other satellite datasets as well. For example, you can use mathematical combinations of bands to generate various indices </a> which can then be used as features in your model. These bands or indices may provide insights into surface characteristics, vegetation, or built-up areas that could influence UHI patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the Sentinel-2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\">To obtain the Sentinel-2 data, we created a GeoTIFF image for a specific date and area of interest, which in this case is the Bronx and Manhattan regions of New York. The GeoTIFF product allows us to extract the relevant band values. In this example, we extracted B01 (Coastal Aerosol), B04 (Red), B06 (Red Edge), and B08 (Near Infrared) values for a single day (24th July, 2021). Participants should review the provided Sentinel-2 sample notebook for common output images (RGB, NDVI) and methods to alter the output.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods of Extracting Band Values from Sentinel-2 Data\n",
    "\n",
    "There are two common methods to extract band values from Sentinel-2 data:\n",
    "\n",
    "<ul>\n",
    "    <li><strong>Using API Calls:</strong> Retrieve band values directly from Sentinel-2 datasets via APIs, such as the <code>planetary_computer</code>.</li>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li><strong>Using GeoTIFF Images:</strong> Create and download a GeoTIFF image containing the desired bands and extract the band values locally. The GeoTIFF image can represent any desired time period (single date or time series mosaic) and include any number of spectral bands.</li>\n",
    "</ul>\n",
    "\n",
    "Participants can select any of these approaches as per their convenience. Since our dataset is large, the API method can be time-consuming and resource-intensive. Therefore, in this sample notebook, we have opted for the second method and extracted the values for bands B01, B04, B08, and B06. Please refer to the Sentinel-2 sample notebook for details about the creation of the GeoTIFF image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(255, 255, 0)\"><strong>Tip 2</strong></h4>\n",
    "<p align=\"justify\"> Rather than extracting the bands for a single day coincident with the ground-based data collection, participants might explore other options to improve data quality. For example, one could select a different single date with minimal or no cloud cover or generate a median mosaic using several scenes within a time series. See the Sentinel-2 sample notebook for examples.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading GeoTIFF Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For building a sample model in this demonstration notebook, we have downloaded a GeoTIFF file locally for a single day (24th July 2021). The file is named <b>S2_sample.tiff</b>. This GeoTIFF file contains values of four bands: Band B01, Band B04, Band B06 and Band B08. In the subsequent section, we will use this GeoTIFF file to extract the band values for the geo-locations given in the training dataset to create the features.\n",
    "\n",
    "First, let’s visualize the bands of the downloaded GeoTIFF image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads and plots four bands (B04, B08, B06, B01) from the GeoTIFF file.\n",
    "\n",
    "# Open the GeoTIFF file\n",
    "tiff_path = \"S2_sample.tiff\"\n",
    "\n",
    "# Read the bands from the GeoTIFF file\n",
    "with rasterio.open(tiff_path) as src1:\n",
    "    band1 = src1.read(1)  # Band [B01]\n",
    "    band2 = src1.read(2)  # Band [B04]\n",
    "    band3 = src1.read(3)  # Band [B06]\n",
    "    band4 = src1.read(4)  # Band [B08]\n",
    "\n",
    "# Plot the bands in a 2x2 grid\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# Flatten the axes for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot the first band (B01)\n",
    "im1 = axes[0].imshow(band1, cmap='viridis')\n",
    "axes[0].set_title('Band [B01]')\n",
    "fig.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Plot the second band (B04)\n",
    "im2 = axes[1].imshow(band2, cmap='viridis')\n",
    "axes[1].set_title('Band [B04]')\n",
    "fig.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Plot the third band (B06)\n",
    "im3 = axes[2].imshow(band3, cmap='viridis')                 \n",
    "axes[2].set_title('Band [B06]')\n",
    "fig.colorbar(im3, ax=axes[2])\n",
    "\n",
    "# Plot the fourth band (B08)\n",
    "im4 = axes[3].imshow(band4, cmap='viridis')\n",
    "axes[3].set_title('Band [B08]')\n",
    "fig.colorbar(im4, ax=axes[3])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(255, 255, 0)\"><strong>Tip 3</strong></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a single point data extraction, participants might explore the approach of creating a focal buffer around the locations (e.g., 50 m, 100 m, 150 m etc). For example, if the specified distance was 50 m and the specified band was “Band 2”, then the value of the output pixels from this analysis would reflect the average values in band 2 within 50 meters of the specific location. This approach might help reduction in error associated with spatial autocorrelation. In this demonstration notebook, we are extracting the band data for each of the locations without creating a buffer zone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Extracting Band Values from the GeoTIFF Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts satellite band values from a GeoTIFF based on coordinates from a csv file and returns them in a DataFrame.\n",
    "\n",
    "def map_satellite_data(tiff_path, csv_path):\n",
    "    \n",
    "    # Load the GeoTIFF data\n",
    "    data = rxr.open_rasterio(tiff_path)\n",
    "    tiff_crs = data.rio.crs\n",
    "\n",
    "    # Read the Excel file using pandas\n",
    "    df = pd.read_csv(csv_path)\n",
    "    latitudes = df['Latitude'].values\n",
    "    longitudes = df['Longitude'].values\n",
    "\n",
    "    # 3. Convert lat/long to the GeoTIFF's CRS\n",
    "    # Create a Proj object for EPSG:4326 (WGS84 - lat/long) and the GeoTIFF's CRS\n",
    "    proj_wgs84 = Proj(init='epsg:4326')  # EPSG:4326 is the common lat/long CRS\n",
    "    proj_tiff = Proj(tiff_crs)\n",
    "    \n",
    "    # Create a transformer object\n",
    "    transformer = Transformer.from_proj(proj_wgs84, proj_tiff)\n",
    "\n",
    "    B01_values = []\n",
    "    B04_values = []\n",
    "    B06_values = []\n",
    "    B08_values = []\n",
    "\n",
    "# Iterate over the latitudes and longitudes, and extract the corresponding band values\n",
    "    for lat, lon in tqdm(zip(latitudes, longitudes), total=len(latitudes), desc=\"Mapping values\"):\n",
    "    # Assuming the correct dimensions are 'y' and 'x' (replace these with actual names from data.coords)\n",
    "    \n",
    "        B01_value = data.sel(x=lon, y=lat,  band=1, method=\"nearest\").values\n",
    "        B01_values.append(B01_value)\n",
    "    \n",
    "        B04_value = data.sel(x=lon, y=lat, band=2, method=\"nearest\").values\n",
    "        B04_values.append(B04_value)\n",
    "        \n",
    "        B06_value = data.sel(x=lon, y=lat, band=3, method=\"nearest\").values\n",
    "        B06_values.append(B06_value)\n",
    "    \n",
    "        B08_value = data.sel(x=lon, y=lat, band=4, method=\"nearest\").values\n",
    "        B08_values.append(B08_value)\n",
    "\n",
    "    # Create a DataFrame with the band values\n",
    "    # Create a DataFrame to store the band values\n",
    "    df = pd.DataFrame()\n",
    "    df['B01'] = B01_values\n",
    "    df['B04'] = B04_values\n",
    "    df['B06'] = B06_values\n",
    "    df['B08'] = B08_values\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping satellite data with training data.\n",
    "final_data = map_satellite_data('S2_sample.tiff', 'Training_data_uhi_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate NDVI (Normalized Difference Vegetation Index) and handle division by zero by replacing infinities with NaN.\n",
    "# See the Sentinel-2 sample notebook for more information about the NDVI index\n",
    "final_data['NDVI'] = (final_data['B08'] - final_data['B04']) / (final_data['B08'] + final_data['B04'])\n",
    "final_data['NDVI'] = final_data['NDVI'].replace([np.inf, -np.inf], np.nan) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining the predictor variables and response variables\n",
    "Now that we have extracted our predictor variables, we need to join them onto the response variable . We use the function <i><b>combine_two_datasets</b></i> to combine the predictor variables and response variables.The <i><b>concat</b></i> function from pandas comes in handy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine two datasets vertically (along columns) using pandas concat function.\n",
    "def combine_two_datasets(dataset1,dataset2):\n",
    "    '''\n",
    "    Returns a  vertically concatenated dataset.\n",
    "    Attributes:\n",
    "    dataset1 - Dataset 1 to be combined \n",
    "    dataset2 - Dataset 2 to be combined\n",
    "    '''\n",
    "    \n",
    "    data = pd.concat([dataset1,dataset2], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining ground data and final data into a single dataset.\n",
    "uhi_data = combine_two_datasets(ground_df,final_data)\n",
    "uhi_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing duplicates\n",
    "Identical or duplicate entries are removed based on specific columns, in our case [ 'B01','B04','B06','B08','NDVI']. This ensures that the dataset is not biased or skewed by repetitive data, allowing the model to train on unique, relevant observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows from the DataFrame based on specified columns and keep the first occurrence\n",
    "columns_to_check = ['B01','B04','B06','B08','NDVI']\n",
    "for col in columns_to_check:\n",
    "    # Check if the value is a numpy array and has more than one dimension\n",
    "    uhi_data[col] = uhi_data[col].apply(lambda x: tuple(x) if isinstance(x, np.ndarray) and x.ndim > 0 else x)\n",
    "\n",
    "# Now remove duplicates\n",
    "uhi_data = uhi_data.drop_duplicates(subset=columns_to_check, keep='first')\n",
    "uhi_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the index of the dataset\n",
    "uhi_data=uhi_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\"> Now let us select the columns required for our model building exercise. We will consider only Band B01, Band B06 and NDVI from the Sentinel-2 data as our predictor variables. It does not make sense to use latitude and longitude as predictor variables, as they do not have any direct impact on predicting the UHI index.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retaining only the columns for B01, B06, NDVI, and UHI Index in the dataset.\n",
    "uhi_data = uhi_data[['B01','B06','NDVI','UHI Index']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\">We will now split the data into 70% training data and 30% test data. Scikit-learn alias “sklearn” is a robust library for machine learning in Python. The scikit-learn library has a <i><b>model_selection</b></i> module in which there is a splitting function <i><b>train_test_split</b></i>. You can use the same.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y), and then into training and testing sets\n",
    "X = uhi_data.drop(columns=['UHI Index']).values\n",
    "y = uhi_data ['UHI Index'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\"> Before initiating the model training we may have to execute different data pre-processing steps. Here we are demonstrating the scaling of B01, B06, NDVI variable by using Standard Scaler.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"justify\">Feature Scaling is a data preprocessing step for numerical features. Many machine learning algorithms like Gradient descent methods, KNN algorithm, linear and logistic regression, etc. require data scaling to produce good results. Scikit learn provides functions that can be used to apply data scaling. Here we are using Standard Scaler. The idea behind Standard Scaler is that it will transform your data such that its distribution will have a mean value 0 and standard deviation of 1.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(255, 255, 0)\"><strong>Tip 4</strong></h4>\n",
    "<p align=\"justify\">There are many data preprocessing methods available, which might help to improve the model performance. Participants should explore various suitable preprocessing methods as well as different machine learning algorithms to build a robust model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the training and test data using standardscaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\">Now that we have the data in a format suitable for machine learning, we can begin training a model. In this demonstration notebook, we will use a random forest regression model from the scikit-learn library. This library offers a wide range of other models, each with extensive parameter tuning and customization capabilities.</p>\n",
    "\n",
    "<p align=\"justify\">Scikit-learn models require the separation of predictor variables and the response variable. We will store the predictor variables (Band B01, B06, NDVI) in array X and the response variable (UHI index) in array Y. It is important not to include the response variable in array X. Additionally, since latitude and longitude do not contribute to the prediction of UHI in this case, we will drop those as well.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Random Forest model on the training data\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\">Now that we have trained our model, the next step is to evaluate its performance. For evaluation, we will use the R² score, a common metric for regression models that measures how well the model explains the variance in the response variable (UHI index). Scikit-learn provides many other metrics that can be used for evaluation, and you can also write custom code for more specific evaluation needs.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Sample Evaluation\n",
    "<p align=\"justify\">We will be evaluating our model's performance using the R² score on the training data. It is important to note that this is in-sample performance testing, which involves evaluating the model on the training dataset. These metrics are not truly indicative of the model's ability to generalize. You should reserve testing on the test data before drawing final conclusions about your model's performance.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\">In this section, we make predictions on the training set and store them in the <b><i>insample_predictions</i></b> variable. The R² score is then calculated to gauge the model's performance on the training data. It is important to keep in mind that this evaluation is for the training set, and further testing on the test set is necessary to assess the model's generalizability.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the training data\n",
    "insample_predictions = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate R-squared score for in-sample predictions\n",
    "Y_train = y_train.tolist()\n",
    "r2_score(Y_train, insample_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-Sample Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating a machine learning model, it is essential to correctly and fairly evaluate the model's ability to generalize. This is because models have a tendency to overfit the dataset they are trained on. To estimate the out-of-sample performance, we will predict on the test data now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "outsample_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate R-squared score for out-sample predictions\n",
    "Y_test = y_test.tolist()\n",
    "r2_score(Y_test, outsample_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are satisfied with your model, you can proceed to make a submission. To do this, you will need to use your model to predict the Urban Heat Island (UHI) index for a set of test coordinates provided in the <b>\"Submission_template.csv\"</b> file and upload the results to the challenge platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the coordinates for the submission\n",
    "test_file = pd.read_csv('Submission_template.csv')\n",
    "test_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping satellite data for submission.\n",
    "val_data = map_satellite_data('S2_sample.tiff', 'Submission_template.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate NDVI (Normalized Difference Vegetation Index) and handle division by zero by replacing infinities with NaN.\n",
    "val_data['NDVI'] = (val_data['B08'] - val_data['B04']) / (val_data['B08'] + val_data['B04'])\n",
    "val_data['NDVI'] = val_data['NDVI'].replace([np.inf, -np.inf], np.nan)  # Replace infinities with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting specific columns (B01, B06, and NDVI) from the validation dataset\n",
    "submission_val_data=val_data.loc[:,['B01','B06','NDVI']]\n",
    "submission_val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling \n",
    "submission_val_data = submission_val_data.values\n",
    "transformed_submission_data = sc.transform(submission_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making predictions\n",
    "final_predictions = model.predict(transformed_submission_data)\n",
    "final_prediction_series = pd.Series(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the results into dataframe\n",
    "submission_df = pd.DataFrame({'Longitude':test_file['Longitude'].values, 'Latitude':test_file['Latitude'].values, 'UHI Index':final_prediction_series.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the sample submission dataframe\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dumping the predictions into a csv file.\n",
    "submission_df.to_csv(\"submission.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload submission file on platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the submission.csv on the <a href =\"https://challenge.ey.com\">platform</a> to get score generated on scoreboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "<div align =\"justify\">Now that you have learned a basic approach to model training, it’s time to try your own approach! Feel free to modify any of the functions presented in this notebook. We look forward to seeing your version of the model and the results. Best of luck with the challenge!</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import common GIS tools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import Planetary Computer tools\n",
    "\n",
    "# Import machine learning tools\n",
    "\n",
    "# Define the bounding box for the entire data region using (Latitude, Longitude)\n",
    "lower_left = (40.75, -74.01)\n",
    "upper_right = (40.88, -73.86)\n",
    "bounds = (lower_left[1], lower_left[0], upper_right[1], upper_right[0])\n",
    "\n",
    "# Define the time window\n",
    "time_window = \"2021-06-01/2021-09-01\"\n",
    "\n",
    "# Search for Landsat data\n",
    "stac = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "landsat_search = stac.search(\n",
    "    bbox=bounds, \n",
    "    datetime=time_window,\n",
    "    collections=[\"landsat-c2-l2\"],\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 50}, \"platform\": {\"in\": [\"landsat-8\"]}},\n",
    ")\n",
    "landsat_items = list(landsat_search.get_items())\n",
    "signed_landsat_items = [planetary_computer.sign(item).to_dict() for item in landsat_items]\n",
    "\n",
    "# Search for Sentinel data\n",
    "sentinel_search = stac.search(\n",
    "    bbox=bounds, \n",
    "    datetime=time_window,\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 30}},\n",
    ")\n",
    "sentinel_items = list(sentinel_search.get_items())\n",
    "signed_sentinel_items = [planetary_computer.sign(item).to_dict() for item in sentinel_items]\n",
    "\n",
    "# Define the pixel resolution for the final product\n",
    "resolution = 10  # meters per pixel \n",
    "scale = resolution / 111320.0 # degrees per pixel for crs=4326 \n",
    "\n",
    "# Load data into xarray DataArray\n",
    "landsat_data = stackstac.stack(signed_landsat_items, epsg=4326, resolution=scale, assets=[\"red\", \"green\", \"blue\", \"nir\", \"st\"])\n",
    "sentinel_data = stackstac.stack(signed_sentinel_items, epsg=4326, resolution=scale, assets=[\"B02\", \"B03\", \"B04\", \"B08\", \"B11\"])\n",
    "\n",
    "# Preprocess data for CNN\n",
    "def preprocess_data(data):\n",
    "    data = data.fillna(0)\n",
    "    data = data.transpose(\"time\", \"y\", \"x\", \"band\")\n",
    "    return data\n",
    "\n",
    "landsat_data = preprocess_data(landsat_data)\n",
    "sentinel_data = preprocess_data(sentinel_data)\n",
    "\n",
    "# Define CNN model\n",
    "def create_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Train CNN model\n",
    "cnn_model = create_cnn_model((landsat_data.shape[1], landsat_data.shape[2], landsat_data.shape[3]))\n",
    "cnn_model.fit(landsat_data, epochs=10, batch_size=32)\n",
    "\n",
    "# Extract features using CNN\n",
    "landsat_features = cnn_model.predict(landsat_data)\n",
    "sentinel_features = cnn_model.predict(sentinel_data)\n",
    "\n",
    "# Combine features\n",
    "features = np.concatenate([landsat_features, sentinel_features], axis=1)\n",
    "\n",
    "# Load UHI dataset\n",
    "uhi_data = ... # Load your UHI dataset here\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, uhi_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
